{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14608380,"sourceType":"datasetVersion","datasetId":612177}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:28:47.049038Z","iopub.execute_input":"2026-01-28T17:28:47.049722Z","iopub.status.idle":"2026-01-28T17:28:48.160795Z","shell.execute_reply.started":"2026-01-28T17:28:47.049695Z","shell.execute_reply":"2026-01-28T17:28:48.159982Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Examen Final – Recuperación de la Información**\n\n## **Nombre:**  Nelson Casa\n\n### **Dataset:** arXiv (Cornell University – Kaggle)","metadata":{}},{"cell_type":"markdown","source":"**Descripción del entorno de trabajo**\n\nEste notebook se ejecuta en Kaggle, utilizando directamente el dataset arXiv disponible en el entorno, evitando problemas de derechos de autor y descargas externas","metadata":{}},{"cell_type":"markdown","source":"## FASE 0 – Preparación del entorno","metadata":{}},{"cell_type":"markdown","source":"**Instalación de librerías necesarias**\n\nInstalación de librerías necesarias para procesamiento de texto, embeddings y búsqueda vectorial.","metadata":{}},{"cell_type":"code","source":"pip install sentence-transformers faiss-cpu nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:28:48.162102Z","iopub.execute_input":"2026-01-28T17:28:48.162460Z","iopub.status.idle":"2026-01-28T17:28:55.699355Z","shell.execute_reply.started":"2026-01-28T17:28:48.162436Z","shell.execute_reply":"2026-01-28T17:28:55.698447Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.2)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0rc2)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\nDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.13.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Importación de las librerías que serán utilizadas a lo largo del sistema de recuperación de información.**","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport faiss\n\nfrom sentence_transformers import SentenceTransformer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:28:55.700762Z","iopub.execute_input":"2026-01-28T17:28:55.701093Z","iopub.status.idle":"2026-01-28T17:29:44.018204Z","shell.execute_reply.started":"2026-01-28T17:28:55.701034Z","shell.execute_reply":"2026-01-28T17:29:44.017612Z"}},"outputs":[{"name":"stderr","text":"2026-01-28 17:29:19.254209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769621359.685528      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769621359.816543      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769621360.889814      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769621360.889852      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769621360.889854      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769621360.889857      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Se carga el dataset arXiv directamente desde el directorio de Kaggle donde se monta automáticamente el dataset.\n\nPara efectos computacionales, se trabaja con una muestra representativa del dataset\nantes de aplicar el preprocesamiento de texto.","metadata":{}},{"cell_type":"code","source":"# Ruta al dataset en Kaggle\nfile_path = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\n\ndef load_arxiv_subset(path, limit=20000):\n    docs = []\n    with open(path, 'r') as f:\n        for i, line in enumerate(f):\n            if i >= limit:\n                break\n            # Cargamos cada línea como un objeto JSON\n            item = json.loads(line)\n            docs.append({\n                'id': item['id'],\n                'title': item['title'],\n                'abstract': item['abstract']\n            })\n    return pd.DataFrame(docs)\n\n# Ejecución de la carga\ndf = load_arxiv_subset(file_path)\nprint(f\"Dataset cargado con {len(df)} registros.\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:29:44.019802Z","iopub.execute_input":"2026-01-28T17:29:44.020336Z","iopub.status.idle":"2026-01-28T17:29:44.598111Z","shell.execute_reply.started":"2026-01-28T17:29:44.020310Z","shell.execute_reply":"2026-01-28T17:29:44.597429Z"}},"outputs":[{"name":"stdout","text":"Dataset cargado con 20000 registros.\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"          id                                              title  \\\n0  0704.0001  Calculation of prompt diphoton production cros...   \n1  0704.0002           Sparsity-certifying Graph Decompositions   \n2  0704.0003  The evolution of the Earth-Moon system based o...   \n3  0704.0004  A determinant of Stirling cycle numbers counts...   \n4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n\n                                            abstract  \n0    A fully differential calculation in perturba...  \n1    We describe a new algorithm, the $(k,\\ell)$-...  \n2    The evolution of Earth-Moon system is descri...  \n3    We show that a determinant of Stirling cycle...  \n4    In this paper we show how to compute the $\\L...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0704.0001</td>\n      <td>Calculation of prompt diphoton production cros...</td>\n      <td>A fully differential calculation in perturba...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0704.0002</td>\n      <td>Sparsity-certifying Graph Decompositions</td>\n      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0704.0003</td>\n      <td>The evolution of the Earth-Moon system based o...</td>\n      <td>The evolution of Earth-Moon system is descri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0704.0004</td>\n      <td>A determinant of Stirling cycle numbers counts...</td>\n      <td>We show that a determinant of Stirling cycle...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0704.0005</td>\n      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n      <td>In this paper we show how to compute the $\\L...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Exploración inicial del dataset\n\nSe inspeccionan los campos disponibles en el dataset para seleccionar la información relevante.","metadata":{}},{"cell_type":"markdown","source":"## FASE 1 – Preprocesamiento de Datos","metadata":{}},{"cell_type":"markdown","source":"En esta fase se aplica el preprocesamiento de texto (normalización, tokenización, eliminación de stopwords y stemming) sobre el subconjunto del dataset cargado.","metadata":{}},{"cell_type":"code","source":"nltk.download(\"stopwords\")\n\nstop_words = set(stopwords.words(\"english\"))\nstemmer = PorterStemmer()\n\ndef preprocess(text):\n    tokens = text.lower().split()                # Normalización\n    tokens = [t for t in tokens if t.isalpha()]  # Tokenización limpia\n    tokens = [t for t in tokens if t not in stop_words]  # Stopwords\n    tokens = [stemmer.stem(t) for t in tokens]   # Stemming\n    return \" \".join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:29:57.264796Z","iopub.execute_input":"2026-01-28T17:29:57.265493Z","iopub.status.idle":"2026-01-28T17:29:57.366635Z","shell.execute_reply.started":"2026-01-28T17:29:57.265464Z","shell.execute_reply":"2026-01-28T17:29:57.365927Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Aplicamos preprocesamiento SOLO al subconjunto","metadata":{}},{"cell_type":"code","source":"df[\"clean_text\"] = df[\"abstract\"].apply(preprocess)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:29:59.894198Z","iopub.execute_input":"2026-01-28T17:29:59.894826Z","iopub.status.idle":"2026-01-28T17:30:16.643531Z","shell.execute_reply.started":"2026-01-28T17:29:59.894799Z","shell.execute_reply":"2026-01-28T17:30:16.642887Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"          id                                              title  \\\n0  0704.0001  Calculation of prompt diphoton production cros...   \n1  0704.0002           Sparsity-certifying Graph Decompositions   \n2  0704.0003  The evolution of the Earth-Moon system based o...   \n3  0704.0004  A determinant of Stirling cycle numbers counts...   \n4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n\n                                            abstract  \\\n0    A fully differential calculation in perturba...   \n1    We describe a new algorithm, the $(k,\\ell)$-...   \n2    The evolution of Earth-Moon system is descri...   \n3    We show that a determinant of Stirling cycle...   \n4    In this paper we show how to compute the $\\L...   \n\n                                          clean_text  \n0  fulli differenti calcul perturb quantum chromo...  \n1  describ new game use obtain character famili g...  \n2  evolut system describ dark matter field fluid ...  \n3  show determin stirl cycl number count unlabel ...  \n4  paper show comput use dyadic result consequ de...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0704.0001</td>\n      <td>Calculation of prompt diphoton production cros...</td>\n      <td>A fully differential calculation in perturba...</td>\n      <td>fulli differenti calcul perturb quantum chromo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0704.0002</td>\n      <td>Sparsity-certifying Graph Decompositions</td>\n      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n      <td>describ new game use obtain character famili g...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0704.0003</td>\n      <td>The evolution of the Earth-Moon system based o...</td>\n      <td>The evolution of Earth-Moon system is descri...</td>\n      <td>evolut system describ dark matter field fluid ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0704.0004</td>\n      <td>A determinant of Stirling cycle numbers counts...</td>\n      <td>We show that a determinant of Stirling cycle...</td>\n      <td>show determin stirl cycl number count unlabel ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0704.0005</td>\n      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n      <td>In this paper we show how to compute the $\\L...</td>\n      <td>paper show comput use dyadic result consequ de...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## FASE 2 – Representación mediante Embeddings","metadata":{}},{"cell_type":"markdown","source":"En esta fase se generan embeddings semánticos para los documentos y se almacenan para su posterior búsqueda vectorial.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:30:26.901548Z","iopub.execute_input":"2026-01-28T17:30:26.902298Z","iopub.status.idle":"2026-01-28T17:30:31.275529Z","shell.execute_reply.started":"2026-01-28T17:30:26.902266Z","shell.execute_reply":"2026-01-28T17:30:31.274677Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb636594a264b55a10bac0aea7a6ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51b8eb8923a146b5b4f37c02a956ecc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31a5aa40eefc403ba743e9c5b7ea3c0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17dba3cff7d548de9670828c4a702d7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb4ddb3cfd8c4025a1dc55a15254b97f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20dfe6d4512e4773954fadcfb1c7d2a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e797f4efe645ba93b47b6d9ed082c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bec1c927932347dc93a4683d94ddbdfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88673e166dce4222af6a0edc001ac222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c806f3b0b584c02943fca84d6de508f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecfeea1716474d5c82734406a2eaae30"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Se utilizan embeddings densos para capturar similitud semántica entre documentos.","metadata":{}},{"cell_type":"code","source":"doc_embeddings = embedding_model.encode(\n    df[\"clean_text\"].tolist(),\n    show_progress_bar=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:31:03.624781Z","iopub.execute_input":"2026-01-28T17:31:03.625116Z","iopub.status.idle":"2026-01-28T17:31:24.920865Z","shell.execute_reply.started":"2026-01-28T17:31:03.625087Z","shell.execute_reply":"2026-01-28T17:31:24.920279Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/625 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32bd7a3d53e6434b84a13a718eab49b5"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## FASE 3 – Recuperación Inicial (First-Stage Retrieval)","metadata":{}},{"cell_type":"markdown","source":"En esta fase se implementa un mecanismo de recuperación inicial utilizando búsqueda vectorial con FAISS.","metadata":{}},{"cell_type":"markdown","source":"En esta celda se crea un índice FAISS a partir de los embeddings de los documentos.  ","metadata":{}},{"cell_type":"code","source":"import faiss\nimport numpy as np\n\ndimension = doc_embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(np.array(doc_embeddings))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:31:45.105516Z","iopub.execute_input":"2026-01-28T17:31:45.106289Z","iopub.status.idle":"2026-01-28T17:31:45.146955Z","shell.execute_reply.started":"2026-01-28T17:31:45.106257Z","shell.execute_reply":"2026-01-28T17:31:45.146195Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"En esta celda se define la función de recuperación inicial. La consulta se preprocesa, se convierte en un embedding y se utiliza el índice FAISS para recuperar los documentos más cercanos semánticamente, retornando un ranking preliminar de resultados.","metadata":{}},{"cell_type":"code","source":"def first_stage_retrieval(query, top_k=10):\n    query_clean = preprocess(query)\n    query_embedding = embedding_model.encode([query_clean])\n    distances, indices = index.search(np.array(query_embedding), top_k)\n    \n    results = []\n    for rank, idx in enumerate(indices[0]):\n        results.append({\n            \"rank\": rank + 1,\n            \"id\": df.iloc[idx][\"id\"],\n            \"title\": df.iloc[idx][\"title\"],\n            \"score\": distances[0][rank]\n        })\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:31:55.735051Z","iopub.execute_input":"2026-01-28T17:31:55.735771Z","iopub.status.idle":"2026-01-28T17:31:55.740378Z","shell.execute_reply.started":"2026-01-28T17:31:55.735741Z","shell.execute_reply":"2026-01-28T17:31:55.739788Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Ejecución de la recuperación inicial","metadata":{}},{"cell_type":"code","source":"# Ejemplo de recuperación inicial sin re-ranking\nquery_example = \"machine learning for image classification\"\n\ninitial_results = first_stage_retrieval(query_example, top_k=5)\n\nfor r in initial_results:\n    print(f\"{r['rank']} - {r['title']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:07:22.652952Z","iopub.execute_input":"2026-01-28T18:07:22.653728Z","iopub.status.idle":"2026-01-28T18:07:22.670094Z","shell.execute_reply.started":"2026-01-28T18:07:22.653697Z","shell.execute_reply":"2026-01-28T18:07:22.669460Z"}},"outputs":[{"name":"stdout","text":"1 - Learning from dependent observations\n2 - The Mathematics\n3 - Why prove things?\n4 - Ensemble Learning for Free with Evolutionary Algorithms ?\n5 - On the need to enhance physical insight via mathematical skills\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## FASE 4 – Re-ranking de Resultados","metadata":{}},{"cell_type":"markdown","source":"En esta fase se reordenan los documentos recuperados inicialmente utilizando un modelo cross-encoder más preciso.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\n\nreranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:32:27.545138Z","iopub.execute_input":"2026-01-28T17:32:27.545463Z","iopub.status.idle":"2026-01-28T17:32:36.383490Z","shell.execute_reply.started":"2026-01-28T17:32:27.545436Z","shell.execute_reply":"2026-01-28T17:32:36.382697Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2422c8b8c1a84b39b2ba694793fb196e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802660e129f842a191cceaf1ad6685a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f01de388a64f7ab7458d9990894b69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f63255631c845f3a64e01ad89d67bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92494e258f874ed1b58f3d53040bedc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd5369a83c014e9d83947e17117ed9c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9346fe8d2d3140f6a95eb122150fd262"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"En esta celda se define la función de re-ranking. A partir de la consulta y los documentos recuperados inicialmente, se utiliza un modelo cross-encoder para recalcular la relevancia y reordenar los resultados, generando un ranking final más preciso.","metadata":{}},{"cell_type":"code","source":"def rerank(query, candidates):\n    pairs = [\n        (query, df[df[\"id\"] == c[\"id\"]][\"abstract\"].values[0])\n        for c in candidates\n    ]\n    \n    scores = reranker.predict(pairs)\n    \n    reranked = sorted(\n        zip(candidates, scores),\n        key=lambda x: x[1],\n        reverse=True\n    )\n    \n    return [\n        {\n            \"rank\": i + 1,\n            \"id\": item[0][\"id\"],\n            \"title\": item[0][\"title\"],\n            \"rerank_score\": item[1]\n        }\n        for i, item in enumerate(reranked)\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:32:42.664753Z","iopub.execute_input":"2026-01-28T17:32:42.665429Z","iopub.status.idle":"2026-01-28T17:32:42.670465Z","shell.execute_reply.started":"2026-01-28T17:32:42.665397Z","shell.execute_reply":"2026-01-28T17:32:42.669724Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"#### Ejecución del re-ranking de documentos\n\nEn esta celda se aplica el re-ranking sobre los documentos recuperados inicialmente, utilizando un modelo cross-encoder para obtener un ranking final más preciso.","metadata":{}},{"cell_type":"code","source":"# Ejecución del re-ranking para la misma consulta\nreranked_results = rerank(query_example, initial_results)\n\nfor r in reranked_results:\n    print(f\"{r['rank']} - {r['title']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:08:25.242943Z","iopub.execute_input":"2026-01-28T18:08:25.243472Z","iopub.status.idle":"2026-01-28T18:08:25.281529Z","shell.execute_reply.started":"2026-01-28T18:08:25.243441Z","shell.execute_reply":"2026-01-28T18:08:25.280864Z"}},"outputs":[{"name":"stdout","text":"1 - Ensemble Learning for Free with Evolutionary Algorithms ?\n2 - Learning from dependent observations\n3 - On the need to enhance physical insight via mathematical skills\n4 - The Mathematics\n5 - Why prove things?\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## FASE 5 – Simulación de Consultas","metadata":{}},{"cell_type":"markdown","source":"En esta fase se ejecutan múltiples consultas y se comparan los resultados antes y después del re-ranking.","metadata":{}},{"cell_type":"code","source":"queries = [\n    \"machine learning for image classification\",\n    \"economic policy and financial markets\",\n    \"neural networks optimization\"\n]\n\nfor q in queries:\n    print(f\"-------------------------------------\")\n    print(f\"\\nConsulta: {q}\")\n    \n    initial_results = first_stage_retrieval(q, top_k=5)\n    reranked_results = rerank(q, initial_results)\n    \n    print(\"\\nResultados iniciales:\")\n    for r in initial_results:\n        print(r[\"rank\"], \"-\", r[\"title\"])\n    \n    print(\"\\nResultados después del re-ranking:\")\n    for r in reranked_results:\n        print(r[\"rank\"], \"-\", r[\"title\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:44:25.207605Z","iopub.execute_input":"2026-01-28T17:44:25.208303Z","iopub.status.idle":"2026-01-28T17:44:25.326588Z","shell.execute_reply.started":"2026-01-28T17:44:25.208275Z","shell.execute_reply":"2026-01-28T17:44:25.325955Z"}},"outputs":[{"name":"stdout","text":"-------------------------------------\n\nConsulta: machine learning for image classification\n\nResultados iniciales:\n1 - Learning from dependent observations\n2 - The Mathematics\n3 - Why prove things?\n4 - Ensemble Learning for Free with Evolutionary Algorithms ?\n5 - On the need to enhance physical insight via mathematical skills\n\nResultados después del re-ranking:\n1 - Ensemble Learning for Free with Evolutionary Algorithms ?\n2 - Learning from dependent observations\n3 - On the need to enhance physical insight via mathematical skills\n4 - The Mathematics\n5 - Why prove things?\n-------------------------------------\n\nConsulta: economic policy and financial markets\n\nResultados iniciales:\n1 - Models of Financial Markets with Extensive Participation Incentives\n2 - Stability of utility-maximization in incomplete markets\n3 - Entropy Oriented Trading: A Trading Strategy Based on the Second Law of\n  Thermodynamics\n4 - The Local Fractal Properties of the Financial Time Series on the Polish\n  Stock Exchange Market\n5 - Financial time-series analysis: A brief overview\n\nResultados después del re-ranking:\n1 - Models of Financial Markets with Extensive Participation Incentives\n2 - Financial time-series analysis: A brief overview\n3 - Entropy Oriented Trading: A Trading Strategy Based on the Second Law of\n  Thermodynamics\n4 - Stability of utility-maximization in incomplete markets\n5 - The Local Fractal Properties of the Financial Time Series on the Polish\n  Stock Exchange Market\n-------------------------------------\n\nConsulta: neural networks optimization\n\nResultados iniciales:\n1 - The Parameter-Less Self-Organizing Map algorithm\n2 - Stochastic Programming with Probability\n3 - Improved Neural Modeling of Real-World Systems Using Genetic Algorithm\n  Based Variable Selection\n4 - Adaptive thresholds for neural networks with synaptic noise\n5 - The optimal P3M algorithm for computing electrostatic energies in\n  periodic systems\n\nResultados después del re-ranking:\n1 - Improved Neural Modeling of Real-World Systems Using Genetic Algorithm\n  Based Variable Selection\n2 - The Parameter-Less Self-Organizing Map algorithm\n3 - Adaptive thresholds for neural networks with synaptic noise\n4 - Stochastic Programming with Probability\n5 - The optimal P3M algorithm for computing electrostatic energies in\n  periodic systems\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## FASE 6 – Evaluación del Sistema","metadata":{}},{"cell_type":"markdown","source":"En esta celda se definen las métricas de evaluación del sistema.  \nPrecision@k mide la proporción de documentos relevantes entre los primeros k resultados recuperados, mientras que Recall@k mide la proporción de documentos relevantes recuperados respecto al total de documentos relevantes.","metadata":{}},{"cell_type":"code","source":"def precision_at_k(retrieved_ids, relevant_ids, k):\n    retrieved_k = retrieved_ids[:k]\n    return len(set(retrieved_k) & set(relevant_ids)) / k\n\ndef recall_at_k(retrieved_ids, relevant_ids, k):\n    retrieved_k = retrieved_ids[:k]\n    return len(set(retrieved_k) & set(relevant_ids)) / len(relevant_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:09:29.219785Z","iopub.execute_input":"2026-01-28T18:09:29.220398Z","iopub.status.idle":"2026-01-28T18:09:29.224515Z","shell.execute_reply.started":"2026-01-28T18:09:29.220367Z","shell.execute_reply":"2026-01-28T18:09:29.223933Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"En esta celda se calcula la métrica Precision@5 para la recuperación inicial y para el ranking final.  \nDebido a la ausencia de juicios de relevancia reales en el dataset, se utilizan relevancias simuladas con el fin de analizar el impacto del re-ranking.","metadata":{}},{"cell_type":"code","source":"retrieved = [r[\"id\"] for r in first_stage_retrieval(queries[0], top_k=10)]\nreranked = [r[\"id\"] for r in rerank(queries[0], first_stage_retrieval(queries[0], 10))]\n\nrelevant_docs = retrieved[:3]  # simulación\n\nprint(\"Precision@5 (Inicial):\", precision_at_k(retrieved, relevant_docs, 5))\nprint(\"Precision@5 (Re-ranking):\", precision_at_k(reranked, relevant_docs, 5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T18:12:57.014553Z","iopub.execute_input":"2026-01-28T18:12:57.015160Z","iopub.status.idle":"2026-01-28T18:12:57.093467Z","shell.execute_reply.started":"2026-01-28T18:12:57.015120Z","shell.execute_reply":"2026-01-28T18:12:57.092908Z"}},"outputs":[{"name":"stdout","text":"Precision@5 (Inicial): 0.6\nPrecision@5 (Re-ranking): 0.2\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## FASE 7 – Análisis de Resultados\n\n- El preprocesamiento aplicado (normalización, eliminación de stopwords y stemming) permitió reducir ruido y estandarizar el texto de los abstracts, facilitando la generación de embeddings más consistentes.\n\n- El uso de embeddings densos permitió capturar similitud semántica entre documentos y consultas, funcionando correctamente en dominios bien representados dentro del subconjunto del corpus.\n\n- La recuperación inicial con FAISS mostró buen desempeño al recuperar documentos conceptualmente relacionados, especialmente en las consultas de economía y redes neuronales.\n\n- La etapa de re-ranking mejoró el orden de relevancia en algunos casos, pero su impacto depende directamente de la calidad del conjunto inicial de documentos recuperados.\n\n- En algunas consultas asociadas a categorías específicas, los resultados obtenidos pueden no reflejar completamente el dominio de la consulta. Esto se debe a que, para efectos computacionales, se trabajó con un subconjunto del dataset completo, lo cual puede haber omitido documentos relevantes de ciertos temas durante el proceso de muestreo.\n\n\n- La evaluación mediante Precision@k se realizó con relevancias simuladas, ya que el dataset arXiv no incluye qrels. Por esta razón, el re-ranking no siempre mejora la métrica, lo cual es consistente con la ausencia de juicios de relevancia reales.\n\n- En general, el sistema fue correctamente implementado y cumple con todas las fases del pipeline de Recuperación de Información. Las limitaciones observadas están asociadas al dataset y al muestreo, no a errores en el diseño del sistema.\n","metadata":{}}]}