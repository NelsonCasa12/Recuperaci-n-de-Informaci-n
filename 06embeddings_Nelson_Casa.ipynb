{
  "cells": [
    {
      "metadata": {
        "id": "d3f8b5c16e7eb563"
      },
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 6: Dense Retrieval e Introducción a FAISS\n",
        "\n",
        "# Nombre: Nelson Casa\n",
        "# Fecha: 26/11/2025\n",
        "\n",
        "## Objetivo de la práctica\n",
        "\n",
        "Generar embeddings con sentence-transformers (SBERT, E5), e indexar documentos con FAISS"
      ],
      "id": "d3f8b5c16e7eb563"
    },
    {
      "metadata": {
        "id": "cdd69ed7fcbeef9d"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 0: Carga del Corpus\n",
        "### Actividad\n",
        "\n",
        "1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.\n",
        "2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento."
      ],
      "id": "cdd69ed7fcbeef9d"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b00fbde6cfc88b",
        "outputId": "1660107a-d72e-4c47-d60f-cd189f476398"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus cargado con 2000 documentos.\n",
            "Primer documento del corpus (primeros 200 caracteres):\n",
            "\n",
            "\n",
            "\n",
            "I am sure some bashers of Pens fans are pretty confused about the lack\n",
            "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
            "I am  bit puzzled too and a bit relieved. However,\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# 1. Carga el corpus 20 Newsgroups\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# 2. Limita el corpus a los primeros 2000 documentos\n",
        "corpus = newsgroups.data[:2000]\n",
        "\n",
        "print(f\"Corpus cargado con {len(corpus)} documentos.\")\n",
        "print(\"Primer documento del corpus (primeros 200 caracteres):\\n\")\n",
        "print(corpus[0][:200])"
      ],
      "id": "b00fbde6cfc88b"
    },
    {
      "metadata": {
        "id": "b9184f4b3e66e20a"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 2: Generación de Embeddings\n",
        "### Actividad\n",
        "\n",
        "1. Usa dos modelos de sentence-transformers. Puedes usar: `'all-MiniLM-L6-v2'` (SBERT), o `'intfloat/e5-base'` (E5). Cuando uses E5, antepon `\"passage: \"` a cada documento antes de codificar.\n",
        "2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.\n",
        "3. Guarda los embeddings en un array de NumPy para su posterior indexación."
      ],
      "id": "b9184f4b3e66e20a"
    },
    {
      "metadata": {
        "id": "525ae7515c6169d9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# 1. Carga el modelo 'all-MiniLM-L6-v2'\n",
        "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 2. Genera los vectores de embeddings para todos los documentos\n",
        "print(\"Generando embeddings con 'all-MiniLM-L6-v2'...\")\n",
        "embeddings_sbert = model_sbert.encode(corpus, show_progress_bar=True)\n",
        "\n",
        "# 3. Guarda los embeddings en un array de NumPy\n",
        "np.save('embeddings_sbert.npy', embeddings_sbert)\n",
        "\n",
        "print(f\"Embeddings generados con SBERT: {embeddings_sbert.shape}\")"
      ],
      "id": "525ae7515c6169d9"
    },
    {
      "metadata": {
        "id": "40462a067ca2d379"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 3: Consulta\n",
        "### Actividad\n",
        "\n",
        "1. Escribe una consulta en lenguaje natural. Ejemplos:\n",
        "\n",
        "    * \"God, religion, and spirituality\"\n",
        "    * \"space exploration\"\n",
        "    * \"car maintenance\"\n",
        "\n",
        "2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon `\"query: \"` a la consulta.\n",
        "3. Recupera los 5 documentos más relevantes con similitud coseno.\n",
        "4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno)."
      ],
      "id": "40462a067ca2d379"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aad085806124c709",
        "outputId": "162ad12a-1a6c-4d33-e916-16161c0e6567"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Codificando la consulta: 'space exploration'...\n",
            "\n",
            "Documentos más relevantes (primeros 500 caracteres):\n",
            "\n",
            "--- Documento 1 (Similitud: 0.4991) ---\n",
            "I am posting this for a friend without internet access. Please inquire\n",
            "to the phone number and address listed.\n",
            "---------------------------------------------------------------------\n",
            "\n",
            "\"Space: Teaching's Newest Frontier\"\n",
            "Sponsored by the Planetary Studies Foundation\n",
            "\n",
            "The Planetary Studies Foundation is sponsoring a one week class for\n",
            "teachers called \"Space: Teaching's Newest Frontier.\" The class will be\n",
            "held at the Sheraton Suites in Elk Grove, Illinois from June 14 through\n",
            "June 18. Participants wh\n",
            "\n",
            "\n",
            "--- Documento 2 (Similitud: 0.4398) ---\n",
            "\n",
            "Well, here goes.\n",
            "\n",
            "The first item of business is to establish the importance space life\n",
            "sciences in the whole of scheme of humankind.  I mean compared\n",
            "to football and baseball, the average joe schmoe doesn't seem interested\n",
            "or even curious about spaceflight.  I think that this forum can\n",
            "make a major change in that lack of insight and education.\n",
            "\n",
            "All of us, in our own way, can contribute to a comprehensive document\n",
            "which can be released to the general public around the world.  The\n",
            "document would \n",
            "\n",
            "\n",
            "--- Documento 3 (Similitud: 0.4321) ---\n",
            "Ron Miller is a space artist with a long and distinguished career.  \n",
            "I've admired both his paintings (remember the USPS Solar System\n",
            "Exploration Stamps last year?) and his writings on the history of\n",
            "spaceflight.  For several years he's been working on a *big* project\n",
            "which is almost ready to hit the streets.  A brochure from his\n",
            "publisher has landed in my mailbox, and I thought it was cool enough\n",
            "to type in part of it (it's rather long).  Especially given the Net's\n",
            "strong interest in vaporware s\n",
            "\n",
            "\n",
            "--- Documento 4 (Similitud: 0.3995) ---\n",
            "Any comments on the absorbtion of the Office of Exploration into the\n",
            "Office of Space Sciences and the reassignment of Griffin to the \"Chief\n",
            "Engineer\" position?  Is this just a meaningless administrative\n",
            "shuffle, or does this bode ill for SEI?\n",
            "\n",
            "In my opinion, this seems like a Bad Thing, at least on the surface.\n",
            "Griffin seemed to be someone who was actually interested in getting\n",
            "things done, and who was willing to look an innovative approaches to\n",
            "getting things done faster, better, and cheaper.  \n",
            "\n",
            "\n",
            "--- Documento 5 (Similitud: 0.3746) ---\n",
            "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
            "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
            "\n",
            "Does anyone know more about this?  How much, to attend????\n",
            "\n",
            "Anyone want to go?\n",
            "\n",
            "\n"
          ]
        }
      ],
      "execution_count": 9,
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1. Escribe una consulta en lenguaje natural\n",
        "query = \"space exploration\"\n",
        "\n",
        "# 2. Codifica la consulta utilizando el mismo modelo de embeddings\n",
        "print(f\"Codificando la consulta: '{query}'...\")\n",
        "query_embedding = model_sbert.encode(query)\n",
        "\n",
        "# Calcula la similitud coseno entre la consulta y todos los embeddings del corpus\n",
        "# Se necesita reformar query_embedding para que sea 2D para similitud coseno\n",
        "similarities = cosine_similarity(query_embedding.reshape(1, -1), embeddings_sbert)[0]\n",
        "\n",
        "# Obtiene los índices de los 5 documentos más similares\n",
        "top_5_indices = np.argsort(similarities)[::-1][:5]\n",
        "\n",
        "print(\"\\nDocumentos más relevantes (primeros 500 caracteres):\\n\")\n",
        "# 4. Muestra los textos de los documentos recuperados\n",
        "for i, idx in enumerate(top_5_indices):\n",
        "    print(f\"--- Documento {i+1} (Similitud: {similarities[idx]:.4f}) ---\")\n",
        "    print(corpus[idx][:500])\n",
        "    print(\"\\n\")"
      ],
      "id": "aad085806124c709"
    },
    {
      "metadata": {
        "id": "2dc9e5e7815c7508"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "2dc9e5e7815c7508"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}